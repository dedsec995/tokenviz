<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tokenizer Comparison Tool</title>
    <!-- Load official tokenizer libraries -->
    <script src="https://unpkg.com/gpt-tokenizer@2.9.0/dist/cl100k_base.js"></script>
    <script src="https://unpkg.com/gpt-tokenizer@2.9.0/dist/o200k_base.js"></script>
    <script src="https://unpkg.com/js-tiktoken@1.0.20/dist/tiktoken.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
        }

        h1 {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 30px;
            font-size: 2.5em;
            font-weight: 700;
            background: linear-gradient(45deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .input-section {
            margin-bottom: 30px;
        }

        .input-label {
            display: block;
            margin-bottom: 10px;
            font-weight: 600;
            color: #34495e;
            font-size: 1.1em;
        }

        .text-input {
            width: 100%;
            padding: 15px;
            border: 2px solid #e0e6ed;
            border-radius: 12px;
            font-size: 16px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            transition: all 0.3s ease;
            background: #f8fafc;
        }

        .text-input:focus {
            outline: none;
            border-color: #667eea;
            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
            background: white;
        }

        .controls {
            display: flex;
            gap: 15px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }

        .tokenizer-toggle {
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 10px 20px;
            background: #f1f5f9;
            border: 2px solid #e2e8f0;
            border-radius: 25px;
            cursor: pointer;
            transition: all 0.3s ease;
            user-select: none;
        }

        .tokenizer-toggle:hover {
            background: #e2e8f0;
            transform: translateY(-2px);
        }

        .tokenizer-toggle.active {
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
            border-color: transparent;
        }

        .tokenizer-toggle input[type="checkbox"] {
            margin: 0;
            transform: scale(1.2);
        }

        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 20px;
        }

        .tokenizer-result {
            background: white;
            border-radius: 15px;
            padding: 20px;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.08);
            border: 1px solid #e2e8f0;
        }

        .tokenizer-title {
            font-size: 1.3em;
            font-weight: 700;
            margin-bottom: 15px;
            color: #2c3e50;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }

        .token-count {
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 600;
        }

        .tokens-display {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            line-height: 1.8;
            word-wrap: break-word;
            background: #f8fafc;
            padding: 15px;
            border-radius: 10px;
            border: 1px solid #e2e8f0;
            max-height: 300px;
            overflow-y: auto;
        }

        .token {
            display: inline-block;
            padding: 2px 6px;
            margin: 1px;
            border-radius: 6px;
            font-size: 14px;
            font-weight: 500;
            border: 1px solid rgba(0, 0, 0, 0.1);
            transition: transform 0.2s ease;
        }

        .token:hover {
            transform: scale(1.05);
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
        }

        .stats {
            display: flex;
            justify-content: space-between;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #e2e8f0;
            font-size: 0.9em;
            color: #64748b;
        }

        .demo-buttons {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }

        .demo-btn {
            padding: 8px 16px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
            border: none;
            border-radius: 20px;
            cursor: pointer;
            font-size: 0.9em;
            transition: all 0.3s ease;
        }

        .demo-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üî§ Tokenizer Comparison Tool</h1>
        
        <div class="input-section">
            <label class="input-label" for="textInput">Enter text to tokenize:</label>
            <div class="demo-buttons">
                <button class="demo-btn" onclick="loadDemo('Hello, world! How are you today?')">Simple Text</button>
                <button class="demo-btn" onclick="loadDemo('The quick brown fox jumps over the lazy dog. This sentence contains various punctuation marks: commas, periods, and colons!')">Complex Text</button>
                <button class="demo-btn" onclick="loadDemo('def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)')">Code Example</button>
                <button class="demo-btn" onclick="loadDemo('üöÄ Emojis and special chars: caf√©, na√Øve, r√©sum√©, ‰∏≠Êñá, ÿßŸÑÿπÿ±ÿ®Ÿäÿ©')">Unicode & Emojis</button>
            </div>
            <textarea 
                id="textInput" 
                class="text-input" 
                rows="4" 
                placeholder="Type or paste your text here..."
                oninput="tokenizeText()"
            >Hello, world! How are you today?</textarea>
        </div>

        <div class="controls">
            <label class="tokenizer-toggle active">
                <input type="checkbox" checked onchange="toggleTokenizer('gpt35')" id="gpt35">
                <span>GPT-3.5 Turbo</span>
            </label>
            <label class="tokenizer-toggle active">
                <input type="checkbox" checked onchange="toggleTokenizer('gpt4')" id="gpt4">
                <span>GPT-4</span>
            </label>
            <label class="tokenizer-toggle active">
                <input type="checkbox" checked onchange="toggleTokenizer('gpt4o')" id="gpt4o">
                <span>GPT-4o</span>
            </label>
            <label class="tokenizer-toggle active">
                <input type="checkbox" checked onchange="toggleTokenizer('davinci')" id="davinci">
                <span>Davinci</span>
            </label>
            <label class="tokenizer-toggle active">
                <input type="checkbox" checked onchange="toggleTokenizer('llama2')" id="llama2">
                <span>Llama 2</span>
            </label>
            <label class="tokenizer-toggle active">
                <input type="checkbox" checked onchange="toggleTokenizer('claude')" id="claude">
                <span>Claude</span>
            </label>
        </div>

        <div class="results-grid" id="resultsGrid">
        </div>
    </div>

    <script>
        // Official tokenizer implementations using external libraries
        let tokenizerInstances = {};
        let isLoading = false;
        
        // Initialize tokenizers when libraries are loaded
        async function initializeTokenizers() {
            if (isLoading) return;
            isLoading = true;
            
            try {
                // Initialize GPT tokenizers using gpt-tokenizer library
                if (typeof GPTTokenizer_cl100k_base !== 'undefined') {
                    tokenizerInstances.gpt35 = GPTTokenizer_cl100k_base;
                    tokenizerInstances.gpt4 = GPTTokenizer_cl100k_base;
                }
                
                if (typeof GPTTokenizer_o200k_base !== 'undefined') {
                    tokenizerInstances.gpt4o = GPTTokenizer_o200k_base;
                }
                
                // Initialize js-tiktoken for other models
                if (typeof jsTiktoken !== 'undefined') {
                    try {
                        const { getEncoding } = jsTiktoken;
                        tokenizerInstances.davinci = getEncoding('p50k_base');
                    } catch (e) {
                        console.log('Davinci tokenizer not available:', e);
                    }
                }
                
                console.log('Initialized tokenizers:', Object.keys(tokenizerInstances));
            } catch (error) {
                console.log('Error initializing tokenizers:', error);
            }
            
            isLoading = false;
        }

        const tokenizers = {
            gpt35: {
                name: "GPT-3.5 Turbo",
                tokenize: async function(text) {
                    if (tokenizerInstances.gpt35) {
                        try {
                            const tokens = tokenizerInstances.gpt35.encode(text);
                            return tokens.map(tokenId => tokenizerInstances.gpt35.decode([tokenId]));
                        } catch (e) {
                            console.log('GPT-3.5 tokenizer error:', e);
                        }
                    }
                    // Fallback approximation
                    return text.split(/(\s+|[^\w\s])/g).filter(t => t.trim())
                        .flatMap(token => {
                            if (token.length > 4 && /^[a-zA-Z]+$/.test(token)) {
                                return [token.slice(0, -2), token.slice(-2)];
                            }
                            return [token];
                        });
                },
                colors: ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57', '#ff9ff3', '#54a0ff', '#5f27cd']
            },
            gpt4: {
                name: "GPT-4",
                tokenize: async function(text) {
                    if (tokenizerInstances.gpt4) {
                        try {
                            const tokens = tokenizerInstances.gpt4.encode(text);
                            return tokens.map(tokenId => tokenizerInstances.gpt4.decode([tokenId]));
                        } catch (e) {
                            console.log('GPT-4 tokenizer error:', e);
                        }
                    }
                    // Fallback approximation
                    return text.split(/(\s+|[.!?;,:])/g).filter(t => t.trim())
                        .flatMap(token => {
                            if (token.length > 6 && /^[a-zA-Z]+$/.test(token)) {
                                const mid = Math.floor(token.length / 2);
                                return [token.slice(0, mid), token.slice(mid)];
                            }
                            return [token];
                        });
                },
                colors: ['#a8e6cf', '#ffd3a5', '#fd9a85', '#c7ceea', '#b8e6b8', '#ffaaa5', '#c5d2ea', '#f4a261']
            },
            gpt4o: {
                name: "GPT-4o",
                tokenize: async function(text) {
                    if (tokenizerInstances.gpt4o) {
                        try {
                            const tokens = tokenizerInstances.gpt4o.encode(text);
                            return tokens.map(tokenId => tokenizerInstances.gpt4o.decode([tokenId]));
                        } catch (e) {
                            console.log('GPT-4o tokenizer error:', e);
                        }
                    }
                    // Fallback approximation - GPT-4o is more efficient
                    return text.split(/(\s+|[.!?;,:])/g).filter(t => t.trim())
                        .flatMap(token => {
                            if (token.length > 8 && /^[a-zA-Z]+$/.test(token)) {
                                const third = Math.floor(token.length / 3);
                                return [token.slice(0, third), token.slice(third, third*2), token.slice(third*2)];
                            }
                            return [token];
                        });
                },
                colors: ['#00d4aa', '#00a8ff', '#7f8c8d', '#2c2c54', '#40407a', '#706fd3', '#f368e0', '#ff3838']
            },
            davinci: {
                name: "Davinci",
                tokenize: async function(text) {
                    if (tokenizerInstances.davinci) {
                        try {
                            const tokens = tokenizerInstances.davinci.encode(text);
                            return tokens.map(tokenId => tokenizerInstances.davinci.decode([tokenId]));
                        } catch (e) {
                            console.log('Davinci tokenizer error:', e);
                        }
                    }
                    // Fallback approximation
                    return text.split(/(\s+|[^\w\s])/g).filter(t => t.trim())
                        .flatMap(token => {
                            if (token.length > 3 && /^[a-zA-Z]+$/.test(token)) {
                                return token.split('').reduce((acc, char, i) => {
                                    if (i % 2 === 0) acc.push(char);
                                    else acc[acc.length - 1] += char;
                                    return acc;
                                }, []);
                            }
                            return [token];
                        });
                },
                colors: ['#667eea', '#764ba2', '#f093fb', '#f5576c', '#4facfe', '#00f2fe', '#43e97b', '#38f9d7']
            },
            llama2: {
                name: "Llama 2",
                tokenize: async function(text) {
                    // Using approximation since LLaMA tokenizer requires specialized setup
                    return text.split(/(\s+)/g).filter(t => t.trim())
                        .flatMap(token => {
                            if (token.startsWith(' ')) return [token];
                            if (token.length > 5 && /^[a-zA-Z]+$/.test(token)) {
                                return ['‚ñÅ' + token.slice(0, 3), token.slice(3)];
                            }
                            return ['‚ñÅ' + token];
                        });
                },
                colors: ['#ff7675', '#fd79a8', '#fdcb6e', '#6c5ce7', '#74b9ff', '#00b894', '#e17055', '#a29bfe']
            },
            claude: {
                name: "Claude",
                tokenize: async function(text) {
                    // Using approximation since Claude tokenizer is proprietary
                    return text.split(/(\s+|[.!?])/g).filter(t => t.trim())
                        .flatMap(token => {
                            if (/^[a-zA-Z]+$/.test(token) && token.length > 4) {
                                const chunks = [];
                                for (let i = 0; i < token.length; i += 3) {
                                    chunks.push(token.slice(i, i + 3));
                                }
                                return chunks;
                            }
                            return [token];
                        });
                },
                colors: ['#e056fd', '#686de0', '#30336b', '#130f40', '#7bed9f', '#70a1ff', '#5352ed', '#40407a']
            }
        };

        let activeTokenizers = new Set(['gpt35', 'gpt4', 'gpt4o', 'davinci', 'llama2', 'claude']);

        function loadDemo(text) {
            document.getElementById('textInput').value = text;
            tokenizeText();
        }

        function toggleTokenizer(tokenizerId) {
            const checkbox = document.getElementById(tokenizerId);
            const label = checkbox.parentElement;
            
            if (checkbox.checked) {
                activeTokenizers.add(tokenizerId);
                label.classList.add('active');
            } else {
                activeTokenizers.delete(tokenizerId);
                label.classList.remove('active');
            }
            
            tokenizeText();
        }

        function getTokenColor(tokenizer, index) {
            return tokenizer.colors[index % tokenizer.colors.length];
        }

        async function tokenizeText() {
            const text = document.getElementById('textInput').value;
            const resultsGrid = document.getElementById('resultsGrid');
            
            if (!text.trim()) {
                resultsGrid.innerHTML = '<p style="text-align: center; color: #64748b; grid-column: 1/-1;">Enter some text to see tokenization results...</p>';
                return;
            }

            // Show loading state
            resultsGrid.innerHTML = '<p style="text-align: center; color: #64748b; grid-column: 1/-1;">üîÑ Tokenizing text...</p>';

            // Initialize tokenizers if not already done
            if (Object.keys(tokenizerInstances).length === 0) {
                await initializeTokenizers();
            }

            resultsGrid.innerHTML = '';

            const promises = Array.from(activeTokenizers).map(async (tokenizerId) => {
                const tokenizer = tokenizers[tokenizerId];
                try {
                    const tokens = await tokenizer.tokenize(text);
                    
                    const resultDiv = document.createElement('div');
                    resultDiv.className = 'tokenizer-result';
                    
                    const tokensHtml = tokens.map((token, index) => {
                        const color = getTokenColor(tokenizer, index);
                        const displayToken = token.replace(/\s/g, '¬∑').replace(/\n/g, '\\n');
                        return `<span class="token" style="background-color: ${color}; color: ${getBestTextColor(color)}" title="Token ${index + 1}: '${token}'">${displayToken}</span>`;
                    }).join('');

                    const avgTokenLength = tokens.length > 0 ? (tokens.reduce((sum, token) => sum + token.length, 0) / tokens.length).toFixed(1) : 0;
                    const compressionRatio = tokens.length > 0 ? (text.length / tokens.length).toFixed(2) : 0;
                    
                    resultDiv.innerHTML = `
                        <div class="tokenizer-title">
                            ${tokenizer.name}
                            <span class="token-count">${tokens.length} tokens</span>
                        </div>
                        <div class="tokens-display">${tokensHtml}</div>
                        <div class="stats">
                            <span>Characters: ${text.length}</span>
                            <span>Avg token length: ${avgTokenLength}</span>
                            <span>Compression ratio: ${compressionRatio}</span>
                        </div>
                    `;
                    
                    return resultDiv;
                } catch (error) {
                    console.error(`Error tokenizing with ${tokenizer.name}:`, error);
                    const errorDiv = document.createElement('div');
                    errorDiv.className = 'tokenizer-result';
                    errorDiv.innerHTML = `
                        <div class="tokenizer-title">
                            ${tokenizer.name}
                            <span class="token-count" style="background: #ff6b6b;">Error</span>
                        </div>
                        <div class="tokens-display" style="color: #ff6b6b;">
                            Unable to load ${tokenizer.name} tokenizer. Using fallback approximation.
                        </div>
                    `;
                    return errorDiv;
                }
            });

            try {
                const results = await Promise.all(promises);
                results.forEach(resultDiv => {
                    if (resultDiv) resultsGrid.appendChild(resultDiv);
                });
            } catch (error) {
                console.error('Error during tokenization:', error);
                resultsGrid.innerHTML = '<p style="text-align: center; color: #ff6b6b; grid-column: 1/-1;">‚ùå Error during tokenization. Please try again.</p>';
            }
        }

        function getBestTextColor(backgroundColor) {
            const hex = backgroundColor.replace('#', '');
            const r = parseInt(hex.substr(0, 2), 16);
            const g = parseInt(hex.substr(2, 2), 16);
            const b = parseInt(hex.substr(4, 2), 16);
            const brightness = (r * 299 + g * 587 + b * 114) / 1000;
            return brightness > 155 ? '#000000' : '#ffffff';
        }

        // Initialize with default text
        window.addEventListener('load', async () => {
            await initializeTokenizers();
            tokenizeText();
        });
    </script>
</body>
</html>